{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa838f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175315db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'omegaconf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m instantiate_from_config\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtaming\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiffusionmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoder, Decoder\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtaming\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvqvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorQuantizer2 \u001b[38;5;28;01mas\u001b[39;00m VectorQuantizer\n",
      "File \u001b[0;32m~/K/vq_opticalflow/main.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OmegaConf\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'omegaconf'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from main import instantiate_from_config\n",
    "\n",
    "from taming.modules.diffusionmodules.model import Encoder, Decoder\n",
    "from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
    "from taming.modules.vqvae.quantize import GumbelQuantize\n",
    "from taming.modules.vqvae.quantize import EMAVectorQuantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4cac48",
   "metadata": {},
   "source": [
    "# 1. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d226cdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVQModel\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModules\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      3\u001b[0m                  ddconfig,\n\u001b[1;32m      4\u001b[0m                  lossconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                  sane_index_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# tell vector quantizer to return indices as bhw\u001b[39;00m\n\u001b[1;32m     14\u001b[0m                  ):\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Modules'"
     ]
    }
   ],
   "source": [
    "\n",
    "class VQModel(torch.nn.Modules):\n",
    "    def __init__(self,\n",
    "                 ddconfig,\n",
    "                 lossconfig,\n",
    "                 n_embed,\n",
    "                 embed_dim,\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 image_key=\"image\",\n",
    "                 colorize_nlabels=None,\n",
    "                 monitor=None,\n",
    "                 remap=None,\n",
    "                 sane_index_shape=False,  # tell vector quantizer to return indices as bhw\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.image_key = image_key\n",
    "        self.encoder = Encoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig)\n",
    "        self.loss = instantiate_from_config(lossconfig)\n",
    "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
    "                                        remap=remap, sane_index_shape=sane_index_shape)\n",
    "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
    "        self.image_key = image_key\n",
    "        if colorize_nlabels is not None:\n",
    "            assert type(colorize_nlabels)==int\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "\n",
    "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
    "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        self.load_state_dict(sd, strict=False)\n",
    "        print(f\"Restored from {path}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        quant, emb_loss, info = self.quantize(h)\n",
    "        return quant, emb_loss, info\n",
    "\n",
    "    def decode(self, quant):\n",
    "        quant = self.post_quant_conv(quant)\n",
    "        dec = self.decoder(quant)\n",
    "        return dec\n",
    "\n",
    "    def decode_code(self, code_b):\n",
    "        quant_b = self.quantize.embed_code(code_b)\n",
    "        dec = self.decode(quant_b)\n",
    "        return dec\n",
    "\n",
    "    def forward(self, input):\n",
    "        quant, diff, _ = self.encode(input)\n",
    "        dec = self.decode(quant)\n",
    "        return dec, diff\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format)\n",
    "        return x.float()\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss = self(x)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            # autoencode\n",
    "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
    "\n",
    "            self.log(\"train/aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "            return aeloss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            # discriminator\n",
    "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
    "            self.log(\"train/discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
    "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
    "            return discloss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss = self(x)\n",
    "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"val\")\n",
    "\n",
    "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"val\")\n",
    "        rec_loss = log_dict_ae[\"val/rec_loss\"]\n",
    "        self.log(\"val/rec_loss\", rec_loss,\n",
    "                   prog_bar=True, logger=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val/aeloss\", aeloss,\n",
    "                   prog_bar=True, logger=True, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        return self.log_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
    "                                  list(self.decoder.parameters())+\n",
    "                                  list(self.quantize.parameters())+\n",
    "                                  list(self.quant_conv.parameters())+\n",
    "                                  list(self.post_quant_conv.parameters()),\n",
    "                                  lr=lr, betas=(0.5, 0.9))\n",
    "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
    "                                    lr=lr, betas=(0.5, 0.9))\n",
    "        return [opt_ae, opt_disc], []\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.conv_out.weight\n",
    "\n",
    "    def log_images(self, batch, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        x = x.to(self.device)\n",
    "        xrec, _ = self(x)\n",
    "        if x.shape[1] > 3:\n",
    "            # colorize with random projection\n",
    "            assert xrec.shape[1] > 3\n",
    "            x = self.to_rgb(x)\n",
    "            xrec = self.to_rgb(xrec)\n",
    "        log[\"inputs\"] = x\n",
    "        log[\"reconstructions\"] = xrec\n",
    "        return log\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        assert self.image_key == \"segmentation\"\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
    "        x = F.conv2d(x, weight=self.colorize)\n",
    "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df00a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'datasets/GOPRO/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1815099",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5adcb0a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (720954812.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    path = os.path.join(data_path,scence)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "gt_paths = []\n",
    "img_paths = []\n",
    "for scence in scenes:\n",
    "    path = os.path.join(data_path,scence)\n",
    "    blur_path = os.path.join(path,'blur')\n",
    "    gt_path = os.path.join(path,'sharp')\n",
    "    images = os.listdir(blur_path)\n",
    "    gt = os.listdir(gt_path)\n",
    "    for idx,_ in enumerate(images):\n",
    "        gt_paths.append(os.path.join(gt_path,images[idx]))\n",
    "        img_paths.append(os.path.join(blur_path,gt[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f9aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gopro_gt.txt','w') as f:\n",
    "    for i in gt_paths:\n",
    "        f.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d39e770d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46266"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a866892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
